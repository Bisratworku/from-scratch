{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96759e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scratch import *\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09164a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets\\\\fashion_mnist_images'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = \"datasets\\\\fashion_mnist_images\"\n",
    "folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "329cca94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = os.listdir(os.path.join(folder, \"train\"))\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe88256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000.png'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shirt = os.listdir(os.path.join(folder, \"train\", \"0\"))\n",
    "shirt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1db0d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(path:str, dataset:str):\n",
    "    label = os.listdir(os.path.join(path, dataset))\n",
    "    X = []\n",
    "    y = []\n",
    "    for lbl in label:\n",
    "        for file in os.listdir(os.path.join(path, dataset, lbl)):\n",
    "             try:  \n",
    "                image = np.array(Image.open(os.path.join(path,dataset, lbl, file)))\n",
    "                X.append(image/255)\n",
    "                y.append(lbl)\n",
    "             except:\n",
    "                continue\n",
    "    return np.array(X), np.array(y).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ce14b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path:str):\n",
    "    X_train, y_train = prepare_dataset(path, \"train\")\n",
    "    X_test, y_test = prepare_dataset(path, \"test\")\n",
    "    return X_train.reshape(-1, 28*28), y_train, X_test.reshape(-1, 28 *28), y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef07fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = create_dataset(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f4f4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(image, label, batch):\n",
    "    key = np.array(range(image.shape[0]))\n",
    "    np.random.shuffle(key)\n",
    "    image = image[key]\n",
    "    label = label[key]\n",
    "    image_batch = np.array_split(image, batch)\n",
    "    label_batch = np.array_split(label, batch)\n",
    "    return image_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5168be37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Loss_CategoricalCrossentropy' object has no attribute 'accumlated_sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mset(\n\u001b[0;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m Loss_CategoricalCrossentropy(),\n\u001b[0;32m      9\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m Optimizer_Adam(decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m),\n\u001b[0;32m     10\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m Accuracy_Categorical()\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mfinalize()\n\u001b[1;32m---> 13\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(X_train, y_train , epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m120\u001b[39m, validation_data \u001b[38;5;241m=\u001b[39m (X_test, y_test))\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\GitHub\\from-scratch\\scratch.py:379\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, X, y, epochs, print_every, validation_data, batch_size)\u001b[0m\n\u001b[0;32m    376\u001b[0m     batch_y \u001b[38;5;241m=\u001b[39m y[step\u001b[38;5;241m*\u001b[39mbatch_size: (step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size]\n\u001b[0;32m    378\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 379\u001b[0m data_loss, regularization_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mcalculate(output, y,include_regularization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    380\u001b[0m loss \u001b[38;5;241m=\u001b[39m data_loss \u001b[38;5;241m+\u001b[39m regularization_loss\n\u001b[0;32m    381\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer_activation\u001b[38;5;241m.\u001b[39mpredictions(\n\u001b[0;32m    382\u001b[0m               output)\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\GitHub\\from-scratch\\scratch.py:211\u001b[0m, in \u001b[0;36mLoss.calculate\u001b[1;34m(self, output, y, include_regularization)\u001b[0m\n\u001b[0;32m    209\u001b[0m sample_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(output, y)\n\u001b[0;32m    210\u001b[0m data_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(sample_losses)\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumlated_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(sample_losses)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccmulated_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sample_losses) \n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m include_regularization:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Loss_CategoricalCrossentropy' object has no attribute 'accumlated_sum'"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.add(Layer_Dense(28*28, 120))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(120, 120))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(120,10))\n",
    "model.set(\n",
    "    loss = Loss_CategoricalCrossentropy(),\n",
    "    optimizer = Optimizer_Adam(decay=1e-5),\n",
    "    accuracy = Accuracy_Categorical()\n",
    ")\n",
    "model.finalize()\n",
    "model.train(X_train, y_train , epochs = 10, batch_size = 120, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "397ced01",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = X_test//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa72fadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000, 784))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b20245d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_image, train_batch_label = shuffle_batch(X_train, y_train,320)\n",
    "test_batch_image, test_batch_label = shuffle_batch(X_test, y_test, 320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dc3a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjest_train(train_batch_img, train_batch_lbl, test_batch_img, test_batch_lbl,* , epochs , print_every, validation_data = ()):\n",
    "    for (tr_img, tr_lbl), (te_img, te_lbl) in zip(zip(train_batch_img, train_batch_lbl),zip(test_batch_img, test_batch_lbl)):\n",
    "        model.train(tr_img, tr_lbl, epochs, print_every, validation_data=dation_data = (te_img,te_lbl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
